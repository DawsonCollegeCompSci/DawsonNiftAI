[INCLUDE=presentation]
Title         : Finish kNN, Introduction to Naive Bayes Algorithm
Sub Title     : 
Author        : Jaya Nilakantan
Affiliation   : Dawson College
Email         : jnilakantan@dawsoncollege.qc.ca
Reveal Theme  : sky
Beamer Theme  : singapore

[TITLE]

# Recap
We've seen 2 algorithms thus far:

* Linear regression: used to predict a value based on a single feature. Assumes a continuous linear relationship

* k-nearest neighbours: used to predict a classification based on feature(s). 

Both algorithms are used with supervised learning: assumes labeled data

# How to choose $k$ in kNN?
We saw that the kNN algorithm is sensitive to the value of $k$

Different results can occur depending on:

* the distribution of the test data points

* the value of $k$

In other words, the accuracy of the results depends on the value of $k$. We need to _tune_ $k$ to acheive the optimal results (i.e., lowest error rates)

# Optimizing $k$
There isn't a general formula to determine the best value of $k$. Instead, you need to tune $k$ for each data set that you are 
examining. 

We need to tune $k$ in the same way that we tune the weights $w_i$

The common methodology used is called *k-fold cross validation*

# k-fold cross validation

* split your training data into a bunch of random, equal sized groups (called *folds*, there are *k* of them. Beware, *k* refers to a
different variable than the *k* in kNN!)

* use one of the folds as the "validation" data, and the remaining folds as the "training" data: run through different values
of k in kNN, and assess the accuracy

* repeat this for every fold (i.e., each fold takes a turn being the validation data)

# Visualization

![k_fold_cv]

[k_fold_cv]: images/k_fold_cv.jpg "k_fold_cv" { width:auto; max-width:90% }


# Pseudo code
```
Initialize value of k-fold
Split test data into k-fold folds
for each fold
  for loop to iterate through different values for kNN's k
    the iterating fold is the validation fold, all remaining folds are training data
    use the data in the validation fold to count the number of errors
  save the accuracy rate for that k
select the k with the best accuracy
```

# How would you use kNN for text classification?

1. Transform the text into numeric vectors

2. use a distance metric to calculate the closest neighbours

kNN classification assumes that similarity can be found be proximity. How can the vectors be formed so that 
proximity is meaningful?

# Example - Bag of Words
One way is to convert the text into associate arrays, where the word is the key, and the value is the frequency
with which the word appeared in the text.

Text: John likes to watch movies. Mary likes movies too.

BoW = {"John":1, "likes":2, "to":1, "watch":1, "movies":2, "Mary":1, "too":1};

The array doesn't necessarily keep the order of the words, just the frequency.

Terms which appear in a text and have a higher frequency in one class of bags over another 
can be thought of as being closer to that bag.

# Naive Bayes classifiers
Naive Bayes classifiers are probabilistic classifiers

Bayes refers to Bayes theorem which defines a relation between conditional probabilities.

Naive refers to the fact that the features are independant (which is a big assumption)

# Bayes Theorem

This [video] has an easy to understand explanation.

[video]: https://www.khanacademy.org/partner-content/wi-phi/wiphi-critical-thinking/wiphi-fundamentals/v/bayes-theorem

# Time for math!

~ Begin Framed { padding:1ex; margin-top:1ex }
The Bayes theorem states

~ Math  {#eq-bayes}
P(A \mid B) = \frac{P(B \mid A) \, P(A)}{P(B)}
~
~ End Framed

# Why is this useful?

Let's say we want to classify e-mails into classes $c$ (e.g., spam, not spam)

There are features $x_i$ (basically the words) that define e-mails 

We know the unconditional probabilities $P(c)$ for each class

* this is simply the frequency with which spam and not-spam appears in our training set


We know the conditional probability $P(x_i \mid c)$ for each feature and class

* this is the frequency of the occurrence of the word in each class

* we interpret this as a measure of how much evidence that particular feature contributes to $c$ being the correct class

# Calculation of $P(c \mid x)$

We want to calculate the probability that a given email is spam. We are interested in:

~ Begin Framed { padding:1ex; margin-top:1ex }
The calculation of the conditional probability that the class is $c$, given the words $x_i$ in the email:

~ Math  {#eq-bayes-complex}
P(c \mid x_1, x_2, ... x_n) = \frac{P(x_1,x_2, ... x_n \mid c) \, P(c)}{P(x_1,x_2, ... x_n)}
~

Notice the denominator doesn't depend on the class, and is effectively constant when I am comparing is a
text belongs to one class or another. So we can ignore it since it doesn't change which class will have the
higher probability

~ End Framed

<!--
# Law of Total Probability

~ Begin Framed { padding:1ex; margin-top:1ex }
This law breaks up a probability into distinct smaller parts. In general terms:

~ Math  {#eq-total-prob1}
P(x) = \sum_{i=1}^n P(x \mid A_i) P(A_i)
~

or, for our purposes
~ Math  {#eq-total-prob2}
P(x_1,x_2, ... x_n) = {\sum_{i=1}^n P(x_1, ... x_n \mid c = i) P(c = i)}
~

~ End Framed
-->
# Now consider the naive part

The Naive Bayes model, which we are using, assumes every feature is independant of each other.

In other words, it assumes $x_i$ are all independant, meaning their probabilities don't impact each other

This means that:

~ Begin Framed { padding:1ex; margin-top:1ex }
Since the features $x_i$ are assumed to be independant

~ Math  {#eq-naive-bayespartial}
P(x_1, ... x_n \mid c) = P(x_1|c)⋅P(x_2|c)⋅⋅⋅P(x_n|c)
~

~ End Framed

# So we can rewrite the Bayes equation:

~ Begin Framed { padding:1ex; margin-top:1ex }

~ Math  {#eq-bayes-complex-2}
P(c = j \mid x_1, ... x_n) = P(x_1|c = i)⋅P(x_2|c = i)⋅⋅⋅P(x_n|c = i) P(c = j)
~
~ End Framed

# Now consider the naive part












Using [Madoko] it is easy to create beautiful presentations.

* [Html][slide]:
  Uses the `Reveal.js` library by [Hakim El Hattab](http://hakim.se).\
  This slide demo in Madoko is an adaption of his online demo.
* [Pdf][slide-pdf]:
  Uses the `Beamer` package for LaTeX
* [Source][slide-mdk]:
  Click to see the source of this presentation.


[madoko]: http://madoko.codeplex.com

## Reveal.js

`reveal.js` is a framework for easily creating beautiful presentations using
HTML. You'll need a browser with support for CSS 3D transforms to see it in
its full glory. 

And any Madoko features just work. Here is some math:

~ Begin Framed { padding:1ex; margin-top:1ex }
A famous equation is $e = mc^2$, but this one is 
famous too:

~ Equation  {#eq-gaussian}
\int_{-\infty}^\infty e^{-a x^2} d x = \sqrt{\frac{\pi}{a}} 
~
~ End Framed

~~ Notes
Oh hey, these are some notes. They'll be hidden in your presentation, but you
can see them if you open the speaker notes window (hit 's' on your keyboard).
~~

## Code

Here is code, highlighted by Madoko 

``` javascript
function sqr( x ) {
  var \(&pi;\) = 3.141593;
  return x*x;  /* the square */
}
```
We used `\(` and `\)` to escape into markdown to write &pi;.

<!-- Example of nested vertical slides -->
~ Begin Vertical { data-background:Gainsboro }

## Vertical Slides {#vertical}
Slides can be nested inside of other slides,
try pressing [down].

[![arrowdown]][down]


[down]: # { .navigate-down }
[arrowdown]: images/arrow.png "Down arrow" { width:178px; height:238px }

## Basement Level 1
Press down or up to navigate.

## Basement Level 2

Use `columns` to put blocks next to each other:

~ Begin Columns
~ Column { width:50% }
A monarch butterfly (shown to the right)
spends the winter in Mexico.
~
~ Column
![butterfly]
~
~ End Columns

[butterfly]: images/butterfly.png "A Monarch butterfly" { width:280px; vertical-align:middle }


<!-- 
For local background images, we need to have a reference to the image
such that it's data gets embedded into the HTML page. We define
it using "display=none" so it is hidden in the presentation. 
We can then refer to the image in "data-background-image". 
-->

![bfly]
[bfly]: images/butterfly.png "Butterfly" { display:none }

## Basement Level 3 { data-background-image:images/butterfly.png }
That's it, time to go back up.

[![arrowup]][back2]

[arrowup]:  images/arrow.png "Up arrow" { width:178px; height:238px; transform:rotate(180deg) }
[back2]:    #vertical { .image }

~ End Vertical


## Themes {#themes}

Reveal.js comes with a few themes built in:

* [Default](?#/themes)
* [Sky](?theme=sky#/themes)
* [Beige](?theme=beige#/themes)
* [Serif](?theme=serif#/themes)
* [Simple](?theme=simple#/themes)
* [Night](?theme=night#/themes)
* [Moon](?theme=moon#/themes)
* [Solarized](?theme=solarized#/themes)

Theme demos are loaded after the presentation which leads to flicker. In
production you should load your theme in the `<head>` using a
`<link>`.

## Transitions { #transitions }

You can select from different transitions, like:\
[Cube](?transition=cube#/transitions) -
[Page](?transition=page#/transitions) -
[Concave](?transition=concave#/transitions) -
[Zoom](?transition=zoom#/transitions) -
[Linear](?transition=linear#/transitions) -
[Fade](?transition=fade#/transitions) -
[None](?transition=none#/transitions) -
[Default](?#/transitions)



## Pauses?

Some pauses.

* {.fragment} One
* {.fragment} Two
* {.fragment} Three

And more:

- Test 1
- Test 2
- Test 3
{.fragmented}

Cool!.

~ Slide
A slide with no header
~


## Point of View

In Reveal.js Press **ESC** to enter the slide overview.

Hold down alt and click on any element to zoom in on it using 
 [zoom.js]. Alt + click anywhere to zoom back out.


[zoom.js]: http://lab.hakim.se/zoom-js


## Works in Mobile Safari

Try it out! You can swipe through the slides and pinch your way to the
overview.


## Printing

You can print a `revealjs` presentation nicely from the browser.

First give the `?print-pdf` or `?print-paper` query on your final
presentation (in the browser address bar) and then print from the Chrome
browser selecting `Save to PDF`.

Read more about it at the [revealjs documentation](https://github.com/hakimel/reveal.js#pdf-export)

## RevealJS library

Normally, the `revealjs` library is loaded from a CDN but you can specify your
own url using metadata:

    Reveal Url: <my url>

This can be useful when using a [server to run revealjs](https://github.com/hakimel/reveal.js#full-setup)
where you may use something like:

    @nopreview Reveal Url: http://localhost:8000/reveal.js


## Thanks for looking :-)

[slide]: http://research.microsoft.com/en-us/um/people/daan/madoko/samples/slidedemo/out/slidedemo.html
[slide-mdk]: https://www.madoko.net/editor.html?#url=http://research.microsoft.com/en-us/um/people/daan/madoko/samples/slidedemo/slidedemo.mdk&options={"delayedUpdate":"true"}
[slide-pdf]: http://research.microsoft.com/en-us/um/people/daan/madoko/samples/slidedemo/out/slidedemo.pdf

# References {-}

https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/#more-on-k

https://en.wikipedia.org/wiki/Bag-of-words_model

https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html

https://srdas.github.io/MLBook/BayesModels.html#bayes-classifier

https://en.wikipedia.org/wiki/Law_of_total_probability

