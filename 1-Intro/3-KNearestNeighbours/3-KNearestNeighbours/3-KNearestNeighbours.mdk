[INCLUDE=style/acmart]
Title       : Exercise - K-Nearest Neighbours
Author      : Jaya Nilakantan
Affiliation : Dawson College
Email       : jnilakantan@dawsoncollege.qc.ca

Logo          : True
Bibliography  : example.bib
Doc class     : [acmlarge]acmart.cls
Bib Style     : style/acm-reference-format.bst


~ HtmlOnly
[TITLE]
~


~ TexOnly
[TITLE]
~


# Introduction     { #sec-intro }
In this exercise, we are introduced to both data sets and a first algorithm in supervised learning. We will be using the famous Iris dataset
for a first algorithm for classification.


## Recall: Supervised learning
Supervised learning start with a set of labeled data (called *training* data) which is used to make a mathematical
or statistical model. The model is used for either prediction or classification purposes.
~Figure { #fig_predict; caption: "Predicting a value based on existing data"; page-align:top}
![suplearning]
~

[suplearning]: images/suplearning.png "suplearning" { width:auto; max-width:90% }

~Figure { #fig_classify; caption: "Classifying an object based on existing data"; page-align:top}
![class]

[class]: images/class.png "class" { width:auto; max-width:90%;}
~
In order to validate the model, *test* data is used. The test data is like the training data, that is chosen to properly
reflect the different cases/demographics/conditions (called *features*) which are being looked at. Training and test data are important:
they drive the correctness of the algorithm (the confidence with which you can make a conclusion).


## Where to find data?
There are various places where you can find data to create and test models:

* repositories of open data
  + websites like Kaggle (kaggle.com)
  + Microsoft Research open data (msropendata.com)
  + government open data (e.g., donnees.ville.montreal.qc.ca)
  + Classic data sets (list at https://en.wikipedia.org/wiki/Data_set#Classic_data_sets)
* APIs
  + for example, Reddit, Stack Overflow, NY Times, Wikipedia, ...
* web scraping

![220px-Iris_versicolor_3]
In this exercise, we are using a famous data set, Fisher's *Iris* flower dataset used for classifying
 irises. Fisher was a statistician and biologist who measured features on three related species of irises. Fun fact - there is a Quebec connection, 2 of the 3
 species were measured from a pasture in the Gaspesie!

[220px-Iris_versicolor_3]: images/220px-Iris_versicolor_3.jpg "220px-Iris_versicolor_3" { width:auto; max-width:90%; float:left; padding-right: 7px }

&br;
# Understanding a classification problem
Consider the example where we ask students prior to taking a test how many hours they slept during the past 12 hours, and how many hours they studied during that same time.
We then get pass/fail results from the test. The results might look something like this *sniff*:

~ Figure { #fig-passfail; caption:"Hours slept and studying vs result"; page-align:top}
![sleep]

[sleep]: images/sleep.PNG "sleep" { width:auto; max-width:90% }

~

It is often easier to visualize the data through a chart. In this case, the colour legend indicates 
if the student passed (blue) or failed (red).
A new point $x$ is added - would you predict that this student passed or failed the exam?


~ Figure { #fig-passfail; caption:"Hours slept and studying vs result"; page-align:top}
![study]
  

[study]: images/study.PNG "study" { width:auto; max-width:90% }
~

## Features and Labels
Every data point in this dataset, just like the salary dataset and the housing prices dataset, has *features* (the variables that influence the result,
i.e., hours studied and hours slept), as well as a *label* (the result). Your instincts in this case tell 
you that the poor students probably failed - not because you were able to draw a line and extrapolate, but because you noticed *clusters*.
With this dataset, each point is discrete. We use the data to try to classify the test data into a particular
discrete class: a student can either pass or fail, noting else.

## K-Nearest Neighbours algorithm
You want to find out if the data point being tested $x$ data point belongs to the Pass cluster or the Fail cluster. We basically want to see which
other training data points it is close to, and guess its classification based on the neighbours. $k$ indicates the number of nearest neighbours you want to calculate to get the vote.
Let's say we say that $k = 3$ for the dataset above. Which are the three closest neighbours to $x$?

The easiest distance measure is *Euclidean*. In our example, we have two features, therefore 2 dimensions. Recall, in two dimensions, if point $p = (p_1, p_2)$ and point $q = (q_1, q_2)$,
then distance between point $p$ and $q$ is:
 
~ Equation {#euclid2D}
d(p,q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2}
~

More generally, when you have *n* dimensions (each representing a feature), then the Euclidean distance between 
2 points is calculated as:
 
~ Equation {#euclidnD}
d(p,q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + ... + (p_n - q_n)^2}
~

We can use Euclidean distance to find the 3 closest example students to the student $x$ at $(4, 5)$ (4 hours studied, 5 hours slept).
We see the three closest points are $(4, 7)$ (distance of 2), $(2, 5)$ (distance of 2), and $(7, 5)$ (distance of 3). Two out of
3 neighbours fails, thuis we would predict that this students fails also.

## How to choose $k$?

**Question**: if the dataset contains $m$ datapoints, does it make sense for $k = m$ ? i.e., should every datapoint
have a vote as a nearest neighbour?

Answer: No! If you do that, you are giving a vote to all datapoints. So if you have an odd number of points, you will predict all
to be one type or another

**Question**: should I choose a small value, like $k=1$?

Answer: No! Your algorithm may become to sensitive to noise/outliers in the data set

There is in fact an optimal value of $k$, where the error rates (false classifications) are minimized. There is a technique called
k-fold cross validation that is used. It is beyond the scope of this exercise, but we will examine it in a future exercise.

## KNN pseudocode

```
Load the data
Initialise the value of $k$
Iterate from 1 to $m$ (total number of training data points)
   Calculate the Euclidean distance between test data and the training datapoint. 
Sort the calculated distances in ascending order
Get top k rows from the sorted list
Count the most frequent classification of these rows
```
       
# Exercise 1

Code the KNN algorithm in a php script by coding the following helper functions:

```
  /** Calculates and returns the Euclidean distance between two points. The points
  are stored in an array, where the elements represent the value of the dimension. You
  may assume that the indices are numeric starting at 0 with no gaps, and that both
  arrays have the same number of elements
  **/
  function euclideanDistance (testpoint: array, datapoint:array)
  
  function sort ()
```



## Iris flower dataset
The flower dataset contains measurements of 3 different but related species of irises. For each species, there are 50
flowers which were measured; and each flower had 4 attributes measured

* sepal length
* sepal width
* petal length
* petal width


# References {-}

http://archive.ics.uci.edu/ml/datasets/Iris

http://caisplusplus.usc.edu/blog/curriculum-supplement/knn

https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/
