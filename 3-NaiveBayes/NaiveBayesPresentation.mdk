[INCLUDE=presentation]
Title         : Finish kNN, Introduction to Naive Bayes Algorithm
Sub Title     : 
Author        : Jaya Nilakantan
Affiliation   : Dawson College
Email         : jnilakantan@dawsoncollege.qc.ca
Reveal Theme  : sky
Beamer Theme  : singapore

[TITLE]

# Recap
We've seen 2 algorithms thus far:

* Linear regression: used to predict a value based on a single feature. Assumes a continuous linear relationship

* k-nearest neighbours: used to predict a classification based on feature(s). 

Both algorithms are used with supervised learning: assumes labeled data

# How to choose $k$ in kNN?
We saw that the kNN algorithm is sensitive to the value of $k$

Different results can occur depending on:

* the distribution of the test data points

* the value of $k$

In other words, the accuracy of the results depends on the value of $k$. We need to _tune_ $k$ to acheive the optimal results (i.e., lowest error rates)

# Optimizing $k$
There isn't a general formula to determine the best value of $k$. Instead, you need to tune $k$ for each data set that you are 
examining. 

We need to tune $k$ in the same way that we tune the weights $w_i$

The common methodology used is called *k-fold cross validation*

# k-fold cross validation

* split your training data into a bunch of random, equal sized groups (called *folds*, there are *k* of them. Beware, *k* refers to a
different variable than the *k* in kNN!)

* use one of the folds as the "validation" data, and the remaining folds as the "training" data: run through different values
of k in kNN, and assess the accuracy

* repeat this for every fold (i.e., each fold takes a turn being the validation data)

# Visualization

![k_fold_cv]

[k_fold_cv]: images/k_fold_cv.jpg "k_fold_cv" { width:auto; max-width:90% }


# Pseudo code
```
Initialize value of k-fold
Split test data into k-fold folds
for each fold
  for loop to iterate through different values for kNN's k
    the iterating fold is the validation fold, all remaining folds are training data
    use the data in the validation fold to count the number of errors
  save the accuracy rate for that k
select the k with the best accuracy
```

# How would you use kNN for text classification?

1. Transform the text into numeric vectors

2. use a distance metric to calculate the closest neighbours

kNN classification assumes that similarity can be found be proximity. How can the vectors be formed so that 
proximity is meaningful?

<!--
# Example - Bag of Words
One way is to convert the text into associate arrays, where the word is the key, and the value is the frequency
with which the word appeared in the text.

Text: John likes to watch movies. Mary likes movies too.

BoW = {"John":1, "likes":2, "to":1, "watch":1, "movies":2, "Mary":1, "too":1};

The array doesn't necessarily keep the order of the words, just the frequency.

Terms which appear in a text and have a higher frequency in one class of bags over another 
can be thought of as being closer to that bag.
-->

# Naive Bayes classifiers
Naive Bayes classifiers are probabilistic classifiers

Bayes refers to Bayes theorem which defines a relation between conditional probabilities.

Naive refers to the fact that the features are independant (which is a big assumption)

# Bayes Theorem

This [video] has an easy to understand explanation.

[video]: https://www.khanacademy.org/partner-content/wi-phi/wiphi-critical-thinking/wiphi-fundamentals/v/bayes-theorem

# Time for math!

~ Begin Framed { padding:1ex; margin-top:1ex }
The Bayes theorem states

~ Math  {#eq-bayes}
P(A \mid B) = \frac{P(B \mid A) \, P(A)}{P(B)}
~
~ End Framed

# Simple [example 1]
Based on the weather (feature), the kids either played outside or not (class). We have collected past data, and want to predict the likelihood of them playing outside given the weather.

![Capture]

1. What are the overall frequencies for each class?
2. What are the estimated probabilities for each class P(c)

[Capture]: images/Capture.PNG "Capture" { width:auto; max-width:90% }

# Continuing [example 1]
If it is sunny, will the kids play outside?
~ Math
\begin{aligned}
P(Yes | Sunny) &= \frac{P (Sunny | Yes) * P (Yes) }{P(Sunny)}\\
&= ((3/9) * (9/14))/(5/14) = 0.6\\
P(No | Sunny) &= \frac{P (Sunny | No) * P (No)}{P(Sunny)} \\
&= ((2/5) * (5/14))/(5/14) = 0.4
\end{aligned}
~
So the children are more likely to play outside.

[Example 1]: https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/

# Application to text classification?

We want to classify e-mails into classes $c$ (e.g., spam, not spam)

There are features $x_i$ (the words) that define e-mails 

We know the unconditional probabilities $P(c)$ for each class

* this is simply the frequency with which spam and not-spam appears in our training set


We know the conditional probability $P(x_i \mid c)$ for each feature and class

* this is the frequency of the occurrence of the word in each class

* this is a measure of how much evidence that particular feature contributes to $c$ being the correct class

# Calculation of $P(c \mid x)$

We want to calculate the probability that a given email is spam. We are interested in:

~ Begin Framed { padding:1ex; margin-top:1ex }
The calculation of the conditional probability that the class is $c$, given the words $x_i$ in the email:

~ Math  {#eq-bayes-complex}
P(c \mid x_1, x_2, ... x_n) = \frac{P(x_1,x_2, ... x_n \mid c) \, P(c)}{P(x_1,x_2, ... x_n)}
~

Notice the denominator doesn't depend on the class, and is effectively constant when I am comparing if a
text belongs to one class or another. So we can ignore it since it doesn't change which class will have the
higher **relative** probability

~ End Framed

<!--
# Law of Total Probability

~ Begin Framed { padding:1ex; margin-top:1ex }
This law breaks up a probability into distinct smaller parts. In general terms:

~ Math  {#eq-total-prob1}
P(x) = \sum_{i=1}^n P(x \mid A_i) P(A_i)
~

or, for our purposes
~ Math  {#eq-total-prob2}
P(x_1,x_2, ... x_n) = {\sum_{i=1}^n P(x_1, ... x_n \mid c = i) P(c = i)}
~

~ End Framed
-->
# Now consider the naive part

The Naive Bayes model, which we are using, assumes every feature is independant of each other.

In other words, it assumes $x_i$ are all independant, meaning their probabilities don't impact each other

This means that:

~ Begin Framed { padding:1ex; margin-top:1ex }
Since the features $x_i$ are assumed to be independant

~ Math  {#eq-naive-bayespartial}
P(x_1, ... x_n \mid c) = P(x_1|c)⋅P(x_2|c)⋅⋅⋅P(x_n|c)
~

~ End Framed

# So we can rewrite the Naive Bayes equation:

~ Begin Framed { padding:1ex; margin-top:1ex }

~ Math  {#eq-bayes-complex-2}
P(c \mid x_1, ... x_n) = P(x_1|c)⋅P(x_2|c)⋅⋅⋅P(x_n|c) P(c)
~
~ End Framed

Note 1: We removed the denominator, which was less that 1 in most cases. This will make our probability estimate smaller than it really is. We don't
mind since we are comparing between 2 option for the higher _relative_ value

Note 2: if the probability of any one term being in the class is zero, the entire term will multiply to 0. To protect us against this, we will use something called *Laplace smoothing*, which adds one.
  

# Let's look at an [example] 

[example]: https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html
Consider this training set:

| DocID | words                               | class |
|:-----------|:------------------------------------|:------|
| 1          | Chinese Beijing Chinese             | China |
| 2          | Chinese Chinese Shanghai            | China |
| 3          | Chinese Macao                       | China |
| 4          | Tokyo Japan Chinese                 | not China |
| 5          | Chinese Chinese Chinese Tokyo Japan | ?     |

Document 1 to 4 are out training set. The words are the features $x$, and the class $c$ is China or Japan.

How would we classify Document 5? China or Not China? Which has a higher probability?

# Applying Naive Bayes

Remember:
~ Begin Framed { padding:1ex; margin-top:1ex }
~ Math  {#eq-bayes-complex-2}
P(c \mid x_1, ... x_n) = P(x_1|c)⋅P(x_2|c)⋅⋅⋅P(x_n|c) P(c)
~
~ End Framed

or 
```
 P(China | "Chinese Chinese Chinese Tokyo Japan" ) 
   = P("Chinese"|China)⋅P("Chinese"|China)⋅P("Chinese"|China)⋅ 
   P("Tokyo"|China)⋅P("Japan"|China)⋅ P(China) 
```
```
P(China) = 3/4 since 3/4 training documents are classified as China
P("Chinese"|China) = (5 occurrences of "Chinese" associated with class China
  plus 1 for Laplace smoothing)/ 
  (8 terms associated with class China + overall vocabulary of 6 unique words)
  = 6/14
P("Tokyo" | China) = 0 occurrences of "Tokyo" associated with class China
  plus 1 for Laplace smoothing)/ 
  (8 terms associated with class China + overall vocabulary of 6 unique words)
  = 1/14
P("Japan" | China) = 0 occurrences of "Japan" associated with class China
  plus 1 for Laplace smoothing)/ 
  (8 terms associated with class China + overall vocabulary of 6 unique words)
  = 1/14
  
```
# China or Not China

P(China | Doc 5) = $(6/14)^3 * 1/14 * 1/14 * 3/4$ ~= 0.0003

<br/>
You can verify on your own:

P(Not China | Doc 5) = $(2/9)^3 * 2/9 * 2/9 * 1/4$ ~= 0.0001

Thus the Naive Bayes classifier will assign "China" as the class for the text since the probability is higher


# Working with a real example

[NY Times] offers a wonderful set of APIs to beautiful labelled data. 

Get an API key, and play around. I am using data extracted previously (by [Hilary Mason]) that has 47
 article snippets related to Arts, and 47 related to Sports.

You will use these as training data for a Naive Bayes classifier, which takes a sentence and decides if it is more
likely to be Arts-related or Sports-related

[NY Times]: https://developer.nytimes.com/
[Hilary Mason]: https://github.com/hmason/ml_class/tree/master/intro_web_data

# NB Pseudo code

$P(Arts) = P(Sports) = 0.5$ 
since we chose equal number of articles. If this is not a good assumption, we should
rethink our training data. 

Since they are the same, we can ignore it in our relative probability calculations.

## Training

Separate the text into words

Ignore words like "the" and "a"

Fill two associate arrays, one for arts, one for sport. 

* key is a word, value is the frequency

* PHP arrays are hashmaps internally, so we will have O(1) access time later

* no need to sort

Count the number of words associated with Arts: sum of all values

Count the number of words associated with Sports: sum of all values

Count the overall number of unique words in the training set: count of all keys

## Testing

Separate the sentence into words

Sum the frequency of the words in Arts plus 1

Sum the frequency of the words in Sports plus 1


Use these to calculate which has a higher probability

# Using PHP-ML

The PHP Machine Learning library comes with a NaiveBayes Classifier class

You provide the training data as an array: so you will have to split the sentences, with a matching labels array.
Training can be done in multiple steps so use a loop.

Once the system is trained, you can predict with an array of the features of the sentence.

# Remember the assumptions with Naive Bayes

1. each feature is independant of the other

2. the probability of a future conditional event is **estimated** by the frequency of past events

# References {-}

https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/#more-on-k

https://en.wikipedia.org/wiki/Bag-of-words_model

https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html

https://srdas.github.io/MLBook/BayesModels.html#bayes-classifier

https://en.wikipedia.org/wiki/Law_of_total_probability

https://en.wikipedia.org/wiki/Naive_Bayes_classifier

https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/

https://github.com/hmason/ml_class/tree/master/intro_web_data

